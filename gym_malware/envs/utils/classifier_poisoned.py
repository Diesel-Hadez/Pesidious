import shap
import math
from torch.utils.data.sampler import SubsetRandomSampler
import torch
import numpy as np
from torch.utils.data import Dataset
import torch.nn as nn
# import pefeatures2
# import pefeatures2malicious
from tqdm import tqdm
import torch.optim as optim
from sklearn.model_selection import train_test_split
import os
import pickle
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

MAX_CAP = 101
MAX_CAP = math.inf
PICKLE_X = 'features_X.pkl'
PICKLE_Y = 'features_Y.pkl'
PICKLE_X_MAL = 'features_X_mal.pkl'

THRESHOLD = 0.8


class PEFileDataset(Dataset):
    def __init__(self, malware_path, benign_path):
        global device
        self.X = []
        self.Y = []
        self.X_mal = []

        if os.path.exists(PICKLE_X):
            file_x = open(PICKLE_X, 'rb')
            self.X = pickle.load(file_x)
            file_x.close()

            file_y = open(PICKLE_Y, 'rb')
            self.Y = pickle.load(file_y)
            file_y.close()

        if os.path.exists(PICKLE_X_MAL):
            file_y = open(PICKLE_X_MAL, 'rb')
            self.X_mal = pickle.load(file_y)
            file_y.close()
            return

        a = 0
        for cur_file in tqdm(os.listdir(malware_path), desc="malware feature loading"):
            a += 1
            if a >= MAX_CAP:
                break
            try:
                with open(malware_path + cur_file, 'rb') as file_handle:
                    bytez = file_handle.read()
                    pe = pefeatures2.PEFeatureExtractor2()
                    rn = RangeNormalize(-0.5, 0.5)
                    state = pe.extract(bytez)
                    state_norm = rn(state)
                    state_norm = torch.from_numpy(
                        state_norm).float().unsqueeze(0).to(device)
                    self.X.append(state_norm)
                    self.Y.append(torch.tensor([[1.0]]))

                    pe = pefeatures2malicious.PEFeatureExtractor2Malicious()
                    rn = RangeNormalize(-0.5, 0.5)
                    state = pe.extract(bytez)
                    state_norm = rn(state)
                    state_norm = torch.from_numpy(
                        state_norm).float().unsqueeze(0).to(device)
                    self.X_mal.append(state_norm)
            except Exception as e:
                print(e)
                continue
        a = 0
        for cur_file in tqdm(os.listdir(benign_path), desc="benign feature loading"):
            a += 1
            if a >= MAX_CAP:
                break
            with open(benign_path + cur_file, 'rb') as file_handle:
                try:
                    bytez = file_handle.read()
                    pe = pefeatures2.PEFeatureExtractor2()
                    rn = RangeNormalize(-0.5, 0.5)
                    state = pe.extract(bytez)
                    state_norm = rn(state)
                    state_norm = torch.from_numpy(
                        state_norm).float().unsqueeze(0).to(device)
                    self.X.append(state_norm)
                    self.Y.append(torch.tensor([[0.0]]))

                    pe = pefeatures2malicious.PEFeatureExtractor2Malicious()
                    rn = RangeNormalize(-0.5, 0.5)
                    state = pe.extract(bytez)
                    state_norm = rn(state)
                    state_norm = torch.from_numpy(
                        state_norm).float().unsqueeze(0).to(device)
                    self.X_mal.append(state_norm)
                except Exception as e:
                    print(e)
                    continue

        file_x = open(PICKLE_X, 'wb')
        pickle.dump(self.X, file_x)
        file_x.close()
        file_y = open(PICKLE_Y, 'wb')
        pickle.dump(self.Y, file_y)
        file_y.close()
        file_y = open(PICKLE_X_MAL, 'wb')
        pickle.dump(self.X_mal, file_y)
        file_y.close()

    def get_x(self, idx):
        return self.X[idx]

    def get_y(self, idx):
        return self.Y[idx]

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        if self.Y[idx].item()== 1.0:
            return self.X_mal[idx], self.Y[idx]
        return self.X[idx], self.Y[idx]


class PoisonedClassifier(nn.Module):
    def __init__(self):
        super(PoisonedClassifier, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(2350, 256),
            nn.ReLU(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        ret = self.layers(x)
        return ret

    def train_classifier(self, malware_path, benign_path):
        model = self
        criterion = nn.BCEWithLogitsLoss()
        optimizer = optim.SGD(model.parameters(), lr=0.0009, momentum=0.1)
        dataloader = PEFileDataset(malware_path, benign_path)

        # https://stackoverflow.com/a/50544887
        batch_size = 30
        validation_split = .2
        shuffle_dataset = True
        random_seed = 12629

        # Creating data indices for training and validation splits:
        dataset_size = len(dataloader)
        indices = list(range(dataset_size))
        split = int(np.floor(validation_split * dataset_size))
        if shuffle_dataset:
            np.random.seed(random_seed)
            np.random.shuffle(indices)
        train_indices, val_indices = indices[split:], indices[:split]

        # Creating PT data samplers and loaders:
        train_sampler = SubsetRandomSampler(train_indices)
        valid_sampler = SubsetRandomSampler(val_indices)

        train_loader = torch.utils.data.DataLoader(
            dataloader, sampler=train_sampler, batch_size=batch_size)
        valid_loader = torch.utils.data.DataLoader(
            dataloader, sampler=valid_sampler, batch_size=batch_size)

        for epoch in range(2):  # loop over the dataset multiple times
            correct = 0
            running_loss = 0.0
            print(len(dataloader))
            for i, data in enumerate(train_loader):
                # get the inputs; data is a list of [inputs, labels]
                inputs, labels = data

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward + backward + optimize
                # print(f"Inputs: {inputs}")
                # print(f"Inputs shape: {inputs.shape}")
                outputs = model(inputs)
                difference = torch.abs(outputs - labels)
                correct += (difference > THRESHOLD).float().sum() / batch_size
                # print(f"Outputs: {outputs.item()}")
                # print(f"Outputs shape: {outputs.shape}")
                # print(f"Labels: {labels.item()}")
                # print(f"Difference: {difference}")
                # print(f"Correct: {correct}")
                # print(f"Labels shape: {labels.shape}")
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                # print(f"Loss: {loss.item()}")
                # print statistics
                running_loss += loss.item()
                if i % 20 == 19:    # print every 2000 mini-batches
                    torch.save(model, "jhc_model")
                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.9f}')
                    running_loss = 0.0
            accuracy = 100 * correct / len(train_loader)
            print(f"Accuracy = {accuracy}")

        torch.save(model, "jhc_model")
        correct = 0
        model.eval()
        for i, data in enumerate(valid_loader):
            inputs, labels = data

            outputs = model(inputs)
            difference = torch.abs(outputs - labels)
            correct += (difference > THRESHOLD).float().sum() / batch_size
        accuracy = 100 * correct / len(valid_loader)
        print(f"Testing Accuracy = {accuracy}")


# normaliza the features


class RangeNormalize(object):
    def __init__(self,
                 min_val,
                 max_val):
        """
        Normalize a tensor between a min and max value
        Arguments
        ---------
        min_val : float
                lower bound of normalized tensor
        max_val : float
                upper bound of normalized tensor
        """
        self.min_val = min_val
        self.max_val = max_val

    def __call__(self, *inputs):
        outputs = []
        for idx, _input in enumerate(inputs):
            _min_val = _input.min()
            _max_val = _input.max()
            a = (self.max_val - self.min_val) / (_max_val - _min_val)
            b = self.max_val - a * _max_val
            _input = (_input * a) + b
            outputs.append(_input)
        return outputs if idx > 1 else outputs[0]


def get_malware_score(bytez):
    global device
    model = torch.load('jhc_model')
    model.eval()
    pe = pefeatures2.PEFeatureExtractor2()

    rn = RangeNormalize(-0.5, 0.5)

    state = pe.extract(bytez)
    state_norm = rn(state)
    state_norm = torch.from_numpy(state_norm).float().unsqueeze(0).to(device)
    ret = model.forward(state_norm)
    return ret

if __name__ == '__main__':
    model = PoisonedClassifier()

    malware_path = "../DikeDataset/files/malware/"
    benign_path = "../DikeDataset/files/benign/"

    model.train_classifier(malware_path, benign_path)

    # model = torch.load('jhc_model')

    dataloader = PEFileDataset(malware_path, benign_path)
    single_loader = torch.utils.data.DataLoader(
        dataloader, batch_size=1, shuffle=True)
    shap_loader = torch.utils.data.DataLoader(
        dataloader, batch_size=1000, shuffle=True)
    batch = next(iter(shap_loader))
    x, _ = batch

    e = shap.GradientExplainer(model, x)

    batch = next(iter(single_loader))

    random_sample, _ = batch

    orig_shap_vals = e.shap_values(random_sample)[0][0]
    shap_vals = np.sort(orig_shap_vals)[::-1]
    # max_val = max(shap_vals)
    # max_ind = np.argmax(shap_vals)

    num_write = 10

    feature_list = [{'name': 'ByteHistogram', 'length': 257, 'start': 0, 'end': 257}, {'name': 'ByteEntropyHistogram', 'length': 256, 'start': 257, 'end': 513}, {'name': 'StringExtractor', 'length': 103, 'start': 513, 'end': 616}, {'name': 'GeneralFileInfo', 'length': 9, 'start': 616, 'end': 625}, {
        'name': 'HeaderFileInfo', 'length': 62, 'start': 625, 'end': 687}, {'name': 'SectionInfo', 'length': 255, 'start': 687, 'end': 942}, {'name': 'ImportsInfo', 'length': 1280, 'start': 942, 'end': 2222}, {'name': 'ExportsInfo', 'length': 128, 'start': 2222, 'end': 2350}]


    for cur_ind in range(num_write):
        cur_val = shap_vals[cur_ind]
        max_ind = np.where(orig_shap_vals == cur_val)[0][0]
        print(f"Importance of {cur_val} at index {max_ind}")
        for feature in feature_list:
            if max_ind >= feature['start'] and max_ind <= feature['end']:
                print("This is feature " + feature['name'])
                break
        print("")
