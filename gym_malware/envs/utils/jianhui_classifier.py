import torch
import numpy as np
from torch.utils.data import Dataset
import torch.nn as nn
import pefeatures2
from tqdm import tqdm
import torch.optim as optim
from sklearn.model_selection import train_test_split
import os
import pickle
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

from torch.utils.data.sampler import SubsetRandomSampler
import math
MAX_CAP = 101
MAX_CAP = math.inf
PICKLE_X = 'features_X.pkl'
PICKLE_Y = 'features_Y.pkl'

THRESHOLD= 0.8

class PEFileDataset(Dataset):
    def __init__(self, malware_path, benign_path):
        global device
        self.X = []
        self.Y = []

        if os.path.exists(PICKLE_X):
            file_x = open(PICKLE_X, 'rb')
            self.X = pickle.load(file_x)
            file_x.close()

            file_y = open(PICKLE_Y, 'rb')
            self.Y = pickle.load(file_y)
            file_y.close()
            return

        a = 0
        for cur_file in tqdm(os.listdir(malware_path), desc="malware feature loading"):
            a += 1
            if a >= MAX_CAP:
                break
            try:
                with open(malware_path + cur_file, 'rb') as file_handle:
                    bytez = file_handle.read()
                    pe = pefeatures2.PEFeatureExtractor2() 
                    rn = RangeNormalize(-0.5, 0.5)
                    state = pe.extract(bytez)
                    state_norm = rn(state)
                    state_norm = torch.from_numpy(state_norm).float().unsqueeze(0).to(device)
                    self.X.append(state_norm)
                    self.Y.append(torch.tensor([[1.0]]))
            except Exception as e:
                print(e)
                continue
        a = 0
        for cur_file in tqdm(os.listdir(benign_path), desc="benign feature loading"):
            a += 1
            if a >= MAX_CAP:
                break
            with open(benign_path + cur_file, 'rb') as file_handle:
                try:
                    bytez = file_handle.read()
                    pe = pefeatures2.PEFeatureExtractor2() 
                    rn = RangeNormalize(-0.5, 0.5)
                    state = pe.extract(bytez)
                    state_norm = rn(state)
                    state_norm = torch.from_numpy(state_norm).float().unsqueeze(0).to(device)
                    self.X.append(state_norm)
                    self.Y.append(torch.tensor([[0.0]]))
                except Exception as e:
                    print(e)
                    continue
        file_x = open(PICKLE_X , 'wb')
        pickle.dump(self.X, file_x)
        file_x.close()

        file_y = open(PICKLE_Y, 'wb')
        pickle.dump(self.Y, file_y)
        file_y.close()
    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.Y[idx]

class JHClassifier(nn.Module):
	def __init__(self):
		super(JHClassifier, self).__init__()
		self.layers = nn.Sequential(
			nn.Linear(2350, 256),
			nn.ReLU(),
			nn.Linear(256, 64),
			nn.ReLU(),
			nn.Linear(64, 1)
		)

	def forward(self, x):
 		ret = self.layers(x)
 		return ret

# normaliza the features
class RangeNormalize(object):
	def __init__(self, 
				 min_val, 
				 max_val):
		"""
		Normalize a tensor between a min and max value
		Arguments
		---------
		min_val : float
			lower bound of normalized tensor
		max_val : float
			upper bound of normalized tensor
		"""
		self.min_val = min_val
		self.max_val = max_val

	def __call__(self, *inputs):
		outputs = []
		for idx, _input in enumerate(inputs):
			_min_val = _input.min()
			_max_val = _input.max()
			a = (self.max_val - self.min_val) / (_max_val - _min_val)
			b = self.max_val- a * _max_val
			_input = (_input * a ) + b
			outputs.append(_input)
		return outputs if idx > 1 else outputs[0]



def get_malware_score(bytez):
    global device
    model = torch.load('jhc_model')
    model.eval()
    pe = pefeatures2.PEFeatureExtractor2() 

    rn = RangeNormalize(-0.5, 0.5)

    state = pe.extract(bytez)
    state_norm = rn(state)
    state_norm = torch.from_numpy(state_norm).float().unsqueeze(0).to(device)
    ret = model.forward(state_norm)
    return ret

def train_me_daddy(malware_path, benign_path):
    model = JHClassifier()
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.0009, momentum=0.1)
    dataloader = PEFileDataset(malware_path, benign_path)


    # https://stackoverflow.com/a/50544887
    batch_size = 16
    validation_split = .2
    shuffle_dataset = True
    random_seed= 12629

    # Creating data indices for training and validation splits:
    dataset_size = len(dataloader)
    indices = list(range(dataset_size))
    split = int(np.floor(validation_split * dataset_size))
    if shuffle_dataset :
        np.random.seed(random_seed)
        np.random.shuffle(indices)
    train_indices, val_indices = indices[split:], indices[:split]

    # Creating PT data samplers and loaders:
    train_sampler = SubsetRandomSampler(train_indices)
    valid_sampler = SubsetRandomSampler(val_indices)


    train_loader = torch.utils.data.DataLoader(dataloader
                                               ,sampler=train_sampler
                                               ,batch_size = batch_size)
    valid_loader = torch.utils.data.DataLoader(dataloader
                                               ,sampler=valid_sampler
                                               ,batch_size = batch_size)

    for epoch in range(2):  # loop over the dataset multiple times
        correct = 0
        running_loss = 0.0
        print(len(dataloader))
        for i, data in enumerate(train_loader):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            # print(f"Inputs: {inputs}")
            # print(f"Inputs shape: {inputs.shape}")
            outputs = model(inputs)
            difference = torch.abs(outputs - labels)
            correct += (difference > THRESHOLD).float().sum() / batch_size
            # print(f"Outputs: {outputs.item()}")
            # print(f"Outputs shape: {outputs.shape}")
            # print(f"Labels: {labels.item()}")
            # print(f"Difference: {difference}")
            # print(f"Correct: {correct}")
            # print(f"Labels shape: {labels.shape}")
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print(f"Loss: {loss.item()}")
            # print statistics
            running_loss += loss.item()
            if i % 20 == 19:    # print every 2000 mini-batches
                torch.save(model, "jhc_model")
                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.9f}')
                running_loss = 0.0
        accuracy  = 100 * correct / len(train_loader)
        print(f"Accuracy = {accuracy}")

    torch.save(model, "jhc_model")
    correct = 0
    for i, data in enumerate(valid_loader):
        inputs, labels = data

        outputs = model(inputs)
        difference = torch.abs(outputs - labels)
        correct += (difference > THRESHOLD).float().sum() / batch_size
    accuracy  = 100 * correct / len(valid_loader)
    print(f"Testing Accuracy = {accuracy}")

train_me_daddy("../../../../DikeDataset/files/malware/", "../../../../DikeDataset/files/benign/")
# model = torch.load('jhc_model')
# model.eval()
